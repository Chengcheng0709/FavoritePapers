#Awesome neural networks  
**Understanding Visual Concepts with Continuation Learning**  
*William F. Whitney, Michael Chang, Tejas Kulkarni, and Joshua B. Tenenbaum*  
[[GitXiv](http://gitxiv.com/posts/x7TJLcz3DEnMsEEGY/understanding-visual-concepts-with-continuation-learning)]  

**Teaching Machines to Read and Comprehend**  
*Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom*  
[[GitXiv](http://gitxiv.com/posts/Hc8LDmzdCP4zfZpDf/teaching-machines-to-read-and-comprehend)]  
[[paper](http://arxiv.org/abs/1506.03340)]  

**Population Code Dynamics in Categorical Perception**  
[[paper](http://www.nature.com/articles/srep22536?utm_content=buffer944df&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)]  

**Information Theoretic-Learning Auto-Encoder**(IJCNN2016)   
[[GitXiv](http://gitxiv.com/posts/GHTJwo72QrFYt6qDS/information-theoretic-learning-auto-encoder)]  

**Data-dependent Initializations of Convolutional Neural Networks**  
*Philipp Krähenbühl, Carl Doersch, Jeff Donahue, Trevor Darrell*  
[[paper](http://arxiv.org/abs/1511.06856)]  
重みの初期化時に各レイヤーの出力を減衰や発散しないように、重みをスケールする。その際、重みの初期値として、入力のk-meansのクラススタリング結果を各フィルタの初期値として使うのが有効[[cite](https://twitter.com/hillbig/status/711426446070194176)]  

**Identity Mappings in Deep Residual Networks**  
*Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun*  
[[paper](http://arxiv.org/abs/1603.05027)]  

**Theano Deep Learning links**  
[[link](https://news.ycombinator.com/item?id=9283105)]  

**A New Method to Visualize Deep Neural Networks**  
*Luisa M. Zintgraf, Taco S. Cohen, Max Welling*  
[[paper](http://arxiv.org/abs/1603.02518)]  

**Create song with neuralnet**  
[[blog](https://maraoz.com/2016/02/02/abc-rnn/)]  

**Why are deep nets reversible: A simple theory, with implications for training**  
*Sanjeev Arora, Yingyu Liang, Tengyu Ma*  
[[paper](http://arxiv.org/abs/1511.05653)]  

**Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks**  
*Tim Salimans, Diederik P. Kingma*  
[[paper](http://arxiv.org/abs/1602.07868)]  

**Group Equivariant Convolutional Networks**  
*Taco S. Cohen, Max Welling*  
[[paper](http://arxiv.org/abs/1602.07576)]  

**Deep Kernel Learning**  
*Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing*  
[[paper](http://arxiv.org/abs/1511.02222)]  

**Understanding deep features with computer-generated imagery**   
*study of decomposition of rendered 3D CAD models fed into a CNN with factors as viewpoint and style*  
[[GitXiv](http://gitxiv.com/posts/rCzAPjX5iCXT5E6cz/understanding-deep-features-with-computer-generated-imagery)]  

**Deep Residual Learning for Image Recognition**   
*Substantially deeper than those used previously*  
[[GitXiv](http://gitxiv.com/posts/LgPRdTY3cwPBiMKbm/deep-residual-learning-for-image-recognition)]  

**Ladder Networks** (NIPS2016)  
[[slideshare](http://www.slideshare.net/eiichimatsumoto106/nips2015-ladder-network)]  
[[blog (with code)](http://rinuboney.github.io/2016/01/19/ladder-network.html)] 

**A Primer on Neural Network Models for Natural Language Processing**  
[[paper](http://arxiv.org/pdf/1510.00726v1.pdf)]  

**Natural Language Understanding with Distributed Representation**  
[[paper](http://arxiv.org/pdf/1511.07916v1.pdf)]  

**Understanding Deep Convolutional Networks**  
[[paper](http://arxiv.org/pdf/1601.04920v1.pdf)]  

**DeepPose**  
[[GitXiv](http://gitxiv.com/posts/McQgsxNvfexkSFghc/deeppose)]  

#useful resources
**Visualizing CNN architectures side by side with mxnet**
[[link](http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/)]  

##Tips
###Explosion suppression for recurrent neural networks  
[[pascanu 2012](http://arxiv.org/pdf/1212.0901.pdf)]  
[[pascanu 2013](http://arxiv.org/pdf/1211.5063.pdf)]  
[[pascanu 2014](http://arxiv.org/pdf/1312.6026.pdf)]  


