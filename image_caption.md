#Image to Caption Generation  
**Zero-Shot Recognition using Dual Visual-Semantic Mapping Paths**  
*Yanan Li, Donghui Wang, Huanhang Hu, Yuetan Lin, Yueting Zhuang*  
[[paper](https://arxiv.org/abs/1703.05002v2)]  

**Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning**  
*Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher*  
[[paper](https://arxiv.org/abs/1612.01887)]  
[[arXivTimes](https://github.com/arXivTimes/arXivTimes/issues/238)]  

**Controllable Text Generation**  
*Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P. Xing*  
[[paper](https://arxiv.org/abs/1703.00955)]  

**Hierarchical Boundary-Aware Neural Encoder for Video Captioning**  
*Lorenzo Baraldi, Costantino Grana, Rita Cucchiara*  
[[paper](https://arxiv.org/abs/1611.09312v2)]  

**Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations**  
*Brent Harrison, Upol Ehsan, Mark O. Riedl*  
[[paper](https://arxiv.org/abs/1702.07826)]  

**Comparative Study of CNN and RNN for Natural Language Processing**  
[[paper](https://arxiv.org/pdf/1702.01923v1.pdf)]  

**Phrase Localization and Visual Relationship Detection with Comprehensive Linguistic Cues**  
*Bryan A. Plummer, Arun Mallya, Christopher M. Cervantes, Julia Hockenmaier, Svetlana Lazebnik*  
[[paper](https://arxiv.org/abs/1611.06641v2)]  

**Reference Based LSTM for Image Captioning(AAAI2017)**  
[[paper](Reference Based LSTM for Image Captioning)]  

**Text-guided Attention Model for Image Captioning(AAAI2017)**  
*Jonghwan Mun, Minsu Cho, Bohyung Han*  
[[paper](https://arxiv.org/abs/1612.03557)]  

**Attention Correctness: Machine Perception vs Human Annotations in Neural Image Captioning(AAAI2017)**  
[[paper]()]  

**ImageNet MPEG-7 Visual Descriptors - Technical Report**  
*Frédéric Rayar*  
[[paper](https://arxiv.org/abs/1702.00187v1)]  

**Learning to Decode for Future Success**  
[[paper](https://arxiv.org/abs/1701.06549)]  
not image captioning, but it may be useful for?

**Incorporating Global Visual Features into Attention-Based Neural Machine Translation**  
[[paper](https://arxiv.org/abs/1701.06521)]  

牛久先生bookmark  
[[link](http://www.ai-gakkai.or.jp/my-bookmark_vol32-no1/?utm_campaign=whats-new&utm_medium=twitter&utm_source=twitter)]  

**Image-Text Multi-Modal Representation Learning by Adversarial Backpropagation**  
*Gwangbeen Park, Woobin Im*  
[[paper](https://arxiv.org/abs/1612.08354)]  


**Visual Storytelling(NAACL2016)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1147.pdf)]  

**Stating the Obvious: Extracting Visual Common Sense Knowledge(NAACL2016)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1023.pdf)]  

**Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings(NAACL2016)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1022.pdf)]  

**Black Holes and White Rabbits:Metaphor Identification with Visual Features(NAACL2016bestlong)**  
[[paper](http://m-mitchell.com/NAACL-2016/NAACL-HLT2016/pdf/N16-1020.pdf)]  



**Rich Image Captioning in the Wild(CVPR2016workshop)**  
*Rich Image Captioning in the Wild*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w12/papers/Tran_Rich_Image_Captioning_CVPR_2016_paper.pdf)]  

**MSR-VTT: A Large Video Description Dataset for Bridging Video and Language(CVPR2016)**  
*Jun Xu , Tao Mei , Ting Yao and Yong Rui*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf)]  

**Jointly Modeling Embedding and Translation to Bridge Video and Language(CVPR2016)**  
*Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, and Yong Rui*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf)]  

**Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks(CVPR2016)**  
*Haonan Yu Jiang Wang Zhiheng Huang Yi Yang Wei Xu*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf)]  

**Unsupervised Learning from Narrated Instruction Videos(CVPR2016)**  
*Jean-Baptiste Alayrac Piotr Bojanowski Nishant Agrawal Josef Sivic*  
[[paper](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Alayrac_Unsupervised_Learning_From_CVPR_2016_paper.pdf)]  

**Movie Description**  
*Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele*  
[[paper](http://arxiv.org/abs/1605.03705)]  

**Sequence to Sequence -- Video to Text**  
*Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko*  
[code](https://github.com/jazzsaxmafia/video_to_sequence)]  

**Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics**  
[[paper](http://www.jair.org/media/3994/live-3994-7274-jair.pdf)]  

**Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures**(arXiv)  
[[paper](http://arxiv.org/pdf/1601.03896.pdf)]  

**Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books** (Arxiv,2015)  
*Yukun Zhu@1, Ryan Kiros@1, Richard Zemel@1, Ruslan Salakhutdinov@1, Raquel Urtasun@1, Antonio Torralba@2, Sanja Fidler@1* (@1:University of Toronto,@2:Massachusetts Institute of Technology)   
[[paper](http://arxiv.org/abs/1506.06724)]
[[code](https://github.com/ryankiros/neural-storyteller)]
[[data](http://www.cs.toronto.edu/~mbweb/)]  

**Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**  
*Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio*  
[[paper](http://jmlr.org/proceedings/papers/v37/xuc15.pdf)]  
[[supplementary](http://jmlr.org/proceedings/papers/v37/xuc15-supp.pdf)]  
[[code](https://github.com/kelvinxu/arctic-captions)]  
[[video introduction](https://www.youtube.com/watch?v=kLWRKr4PT_E)]  
[[slide](http://www.slideshare.net/eunjileee/show-attend-and-tell-neural-image-caption-generation-with-visual-attention)]  
[[chainer](https://github.com/apple2373/chainer_caption_generation)]  

**Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models** (TACL, 2015)  
*Ryan Kiros, Ruslan Salakhutdinov, Richard Zemel*  
[[paper](http://arxiv.org/abs/1411.2539)]  
[[demo](http://deeplearning.cs.toronto.edu/i2thttp://deeplearning.cs.toronto.edu/i2t)]  
[[code](https://github.com/ryankiros/visual-semantic-embedding)]  

**Zero-Shot Learning by Convex Combination of Semantic Embeddings**(ICLR2014)    
[[paper](https://arxiv.org/pdf/1312.5650.pdf)]    

**DeViSE: A Deep Visual-Semantic Embedding Model**(NIPS2013)  
[[paper](https://papers.nips.cc/paper/5204-devise-a-deep-visual-semantic-embedding-model)]  
[[slide(ja)(http://www.slideshare.net/beam2d/nips2013-devise)]  

##Survey
[[link](http://www.slideshare.net/metaps_JP/deep-learning-50383383)]  
[[link2](http://www.slideshare.net/YoshitakaUshiku/ss-57148161)]  
**A Survey of Current Datasets for Vision and Language Research**  
[[paper](http://cs.rochester.edu/~nasrinm/files/Papers/Survey-Vision-and-Language.pdf)]  

##generation from root word  
**A Deep Learning Approach for Arabic Caption Generation using Roots-Words(AAAI2017)**  

