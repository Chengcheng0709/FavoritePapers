#Optimization

**Dropout distillation(ICML2016)**  
*Samuel Rota Bul√≤, Lorenzo Porzi, Peter Kontschieder*  
[[paper](http://jmlr.org/proceedings/papers/v48/bulo16.pdf)]  
[[supplement](http://jmlr.org/proceedings/papers/v48/bulo16-supp.pdf)]  

**Learning to learn by gradient descent by gradient descent(ArXiv)**  
*Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W. Hoffman, David Pfau, Tom Schaul, Nando de Freitas*  
[[paper](http://arxiv.org/abs/1606.04474)]  

**Controlling Exploration Improves Training for Deep Neural Networks**  
*Yasutoshi Ida, Yasuhiro Fujiwara, Sotetsu Iwamura(NTT)*  
[[paper](https://arxiv.org/abs/1605.09593)]  

**Convex Relaxation Regression: Black-Box Optimization of Smooth Functions by Learning Their Convex Envelopes**(Arxiv2016)    
[[paper](http://arxiv.org/pdf/1602.02191v2.pdf)  

**Adam: A Method for Stochastic Optimization** (ICLR2015)   
*D.P. Kingma, M. Welling* (Universiteit van Amsterdam)  
[[paper](http://arxiv.org/abs/1412.6980)]  

**Accelerating Asymptotically Exact MCMC for Computationally Intensive Models via Local Approximations**  (Arxiv,2015)  
*Patrick R. Conrad, Youssef M. Marzouk, Natesh S. Pillai, Aaron Smith*  
[[paper](http://arxiv.org/abs/1402.1694)]  

**Taking the Human Out of the Loop: A Review of Bayesian Optimization**  
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P. Adams and Nando de Freitas  
[[paper](https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf)  





